{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Political Affiliation Classification "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "- Cameron Faulkner\n",
    "- Nikhil Hegde\n",
    "- Qianxi Gong\n",
    "- Atul Nair"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "##### Problem Statement\n",
    "Twitter has become a popular platform for political discourse, where people express their views on a variety of issues. Given the increasing polarization in the political landscape, it is of great interest to understand how people's political views are reflected in their online behavior. Our goal is to develop a model that accurately predicts whether a tweet is posted by a Republican or Democrat, taking into account both the textual content of the tweet and associated numerical features such as the number of retweets, likes, and followers. We seek to identify the best set of features and models that can effectively capture the relationship between the text and numerical features and political affiliation of tweet authors.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "Based on existing literature , Several studies have shown promising results using supervised learning models for political affiliation classification based on Twitter data. The results of these studies showed that some models can predict partisan affiliation, with accuracy rates ranging from around 60% to over 90% depending on the specific model and dataset used. For example, in a study by Conover et al. \n",
    "(2011), a logistic regression model achieved an accuracy of 86.4% in predicting political orientation (liberal vs. conservative) based on Twitter data [1]. In another study by Barberá et al. (2015), a Random Forest model achieved an accuracy of 87% in predicting US congressional districts' partisan affiliation [2]. They found that incorporating information on retweets and mentions, in addition to tweet text, significantly improved the accuracy of their models [2]. This suggests that the choice of features can play a critical role in the effectiveness of these models.However, an unanswered question in this field is the generalizability of these models to different political contexts or populations. For instance, a study by Mocanu et al. (2015) showed that supervised learning models trained on tweets from the UK Independence Party (UKIP) members may not generalize well to other political groups, such as the Labour Party [3]. Similarly, the effectiveness of these models may vary across different geographical regions, cultures, or languages.\n",
    "\n",
    "Therefore, while the literature shows promising results for partisan affiliation prediction using supervised machine learning on Twitter, it is important to take into consideration the specific political context and the selection of features to ensure the generalizability and robustness of these models.\n",
    "We looked at existing research for background information , but our ideas were mostly internally conceptualized. On obtaining the dataset , which had several excellent features , we managed to use our own intuition to build and train the models. The studies cited were mostly background information to give us an idea about the possible pitfalls of our own model\n",
    "\n",
    "\n",
    "[1] Conover, M. D., Ratkiewicz, J., Francisco, M., Gonçalves, B., Menczer, F., & Flammini, A. (2011). Political polarization on twitter. In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media (pp. 89-96).\n",
    "[2] Pak, A., & Paroubek, P. (2010). Twitter as a corpus for sentiment analysis and opinion mining. In LREc (pp. 1320-1326).\n",
    "[3] Mocanu, D., Baronchelli, A., Perra, N., Gonçalves, B., Zhang, Q., & Vespignani, A. (2015). The Twitter of Babel: Mapping world languages through microblogging platforms. PloS one, 10(11), e0142968. doi: 10.1371/journal.pone.0142968"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Our goal is to develop a Machine Learning model that predicts whether a tweet is posted by a Republican or Democrat, taking into account both the textual content of the tweet using Semantic Analysis and NLP motivated techniques and associated numerical features such as the number of retweets, likes, and followers. We seek to identify the best set of features and models that can effectively capture the relationship between the text and numerical features and political affiliation of tweet authors which would result in the most accurate predictions using out Evaluation Metrics described below. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "In order to train our predictive model, we scraped every Tweet made by a member of the American Congress with a Twitter account during the 117th session of Congress, which ran from January 3rd, 2021 to January 3rd, 2023. In total, 530 of 535 members of this session of Congress had Twitter accounts.\n",
    "\n",
    "We are aware this small collection of users could bias the data. If we were to work with a subset of random Twitter users instead, it would be difficult to manually identify hundreds of Twitter accounts whether they belong to an American political party, or show liberal or conservative viewpoints. Additionally, filtering through bot accounts pose another difficult challenge. \n",
    "\n",
    "Gathering data from verified Twitter accounts of members of congress allows us to easily identify political leanings as well and we can extrapolate our findings to the rest of the population as these members of Congress are the public figures of the party. \n",
    "\n",
    "Using the dataset of Congressional Twitter handles, we utilized the snscrape Python package to scrape all of the Tweets in our specified time frame from each of the members. This amounted to 786268 Tweets (clearly, members of Congress are prolific Tweeters). Link to the unprocessed scraped Tweets: https://drive.google.com/file/d/1IUHw2ktOTZc41vi_7Ijrw20-maLJfjTU/view?usp=share_link\n",
    "\n",
    "The median amount of Tweets sent by a member of Congress over this span was 1336, corresponding to approximately 1.83 Tweets per day. \n",
    "\n",
    "For each Tweet in our dataset, we kept the following variables and eliminated some we found irrelevant:\n",
    "\n",
    "KEPT VARIABLES:\n",
    "\n",
    "-rawContent - the content of the tweet itself generated by the twitter user \n",
    "\n",
    "-replyCount - number of replies\n",
    "\n",
    "-retweetCount - number of retweets \n",
    "\n",
    "-likeCount - number of likes \n",
    "\n",
    "-quoteCount - number of quotes \n",
    "\n",
    "-links \n",
    "\n",
    "-Media\n",
    "\n",
    "-mentionedUsers - the users mentioned in the tweet \n",
    "\n",
    "-Hashtags\n",
    "\n",
    "-Party (we collected this by applying the party identification of each individual member of Congress to their Tweets of Congress while scraping them)\n",
    "\n",
    "We elected to drop the following variables as we found them to be redundant or of little utility to our project:\n",
    "\n",
    "DROPPED VARIABLES:\n",
    "\n",
    "-Url\n",
    "\n",
    "-Date\n",
    "\n",
    "-Id\n",
    "\n",
    "-User\n",
    "\n",
    "-renderedContent\n",
    "\n",
    "-Coordinates\n",
    "\n",
    "-place\n",
    "\n",
    "-Card\n",
    "\n",
    "-viewCount\n",
    "\n",
    "-Vibe\n",
    "\n",
    "\n",
    "\n",
    "However, we only used these variables as a starting point for our analysis and needed to treat some of them so that they were usable in our dataset. \n",
    "\n",
    "\n",
    "##### Preprocessing and Addition of Features\n",
    "TREATMENT OF VARIABLES TAKEN FROM TWITTER SCRAPING:\n",
    "\n",
    "While all of our variables with the word “Count” in them were already in numerical format, we needed to transform the others into numerical data.\n",
    "\n",
    "For the variables “links”, “Media”, “mentionedUsers”, and “Hashtags”, we weren’t interested in their actual content, but rather how many of them were present in a given tweet (e.g. the number of links placed in a single Tweet). As such, we transformed each variable for every observation into the length of the list of items it pertained to yielding values from 0 and up. \n",
    "\n",
    "For “Party” we changed our initial classification of “R” or “D” into 1 and 0, respectively, as we knew that we cannot feed a model categorical data in the form of text. We will be using the party label as the basis of our classifier.\n",
    "\n",
    "As a fortunate result of using this particular scraper, all of our data was well formatted (no strings were in fields reserved for floats, for example) and, after doing our best practice checks on our data, we found no observations needed to be thrown out.\n",
    "\n",
    "link to notebook with implementation of cleaning process described before: https://github.dev/COGS118A/Group040-Wi23/blob/7ee24d6fe6ac05b2b2f63b75e9e21c35270c9291/tweet_cleaning.ipynb\n",
    "\n",
    "\n",
    "ADDITION OF DERIVED VARIABLES:\n",
    "\n",
    "Sentiment Analysis:\n",
    "\n",
    "In addition to the variables provided by our scraping, we conducted sentiment analysis on the text content of each of our Tweets with the theory that Republicans and Democrats may differ in the tone of their communication with the public. \n",
    "\n",
    "To apply our sentiment analysis, we took the rawContent variable for each of our Tweets (literally the text of the Tweet) and applied the Vader Sentiment analysis algorithm to them. This algorithm is specifically for the sentiment classification of social media posts, an important factor to consider given how different a Tweet is from a novel. We used this model to classify the sentiment of each Tweet as “positive”, “negative”, or “neutral” based on the scores provided by the algorithm and the thresholds recommended by the developers of the sentiment analysis package. \n",
    "\t\n",
    "Again, we ran into the problem of using categorical data in an ML algorithm and so we used one-hot encoding to create three separate binary variables corresponding to each of the three sentiments. \n",
    "\n",
    "link to notebook with sentiment analysis addition: https://github.dev/COGS118A/Group040-Wi23/blob/7ee24d6fe6ac05b2b2f63b75e9e21c35270c9291/sentiment%20analysis%20addition.ipynb\n",
    "\n",
    "Word Frequency and Importance:\n",
    "\n",
    "We also decided to take further advantage of the rich text content of our dataset and include the frequency and uniqueness of the words that comprised the Tweets in our dataset under the hypothesis that Republicans and Democrats used certain words more frequently than the other party. \n",
    "\n",
    "To do so, we used the standard Term Frequency-Inverse Document Frequency measure to quantify the significance of the words used in our Tweets. Understanding that using the algorithm on unprocessed data can yield falsely significant results, we knew we must preprocess the data before applying TF-IDF. We removed stop words, words that commonly occur and offer no value to our classification, using the Natural Language Tool Kit’s (nltk) premade stopwords corpus, and also applied nltk’s Lemmatizer which removes the conjugation of words to avoid duplicates and diluting the significance of them. \n",
    "\n",
    "We then took our processed Tweets and vectorized them with sklearn’s TfidfVectorizer and extracted 4500 features from our dataset. The resulting matrix was in the form of columns that contained the TF_IDF scores for each of the 4500 features, for every one of the Tweets. This matrix, added to our 11 other variables, formed our completed dataset.\n",
    "\n",
    "link to notebook with text processing and vectorization, and model application: https://github.dev/COGS118A/Group040-Wi23/blob/2a99aec348b73c5eda78ccdd1974db821d0badca/algorithm_application.ipynb\n",
    "\t\n",
    "COMPLETED DATASET:\n",
    "\n",
    "In sum, the dataset we are using for our model has the dimensions of (786268, 4513). This includes the following features: sentiment, engagement metrics from Twitter users, word similarity, and party labels from the Member of Congress who made a given Tweet that will serve as our classification labels.\n",
    "\n",
    "Link to completed dataset on Google Drive (having difficulty adding file to Github due to size): https://drive.google.com/file/d/1Z5hXQE7qe--6Cpk_8biCfP_sHRZy7XXb/view?usp=sharing\n",
    "\n",
    "Sources/Packages Used:\n",
    "\n",
    "- Congressional Twitter Handles: https://frac.org/wp-content/uploads/MOC_Twitter-Handles_117th.pdf\n",
    "\n",
    "- VADER Sentiment Analysis Documentation: https://github.com/cjhutto/vaderSentiment\n",
    "\n",
    "- NLTK Stopwords Documentation: https://www.nltk.org/search.html?q=stopwords&check_keywords=yes&area=default\n",
    "\n",
    "- NLTK Lemmatizer Documentation: https://www.nltk.org/_modules/nltk/stem/wordnet.html\n",
    "\n",
    "- SKLearn TFidfVectorizer Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "##### Model Development \n",
    "!!! TO BE UPDATED !!!\n",
    "We will compare the performance of different algorithms, such as logistic regression, decision trees, and support vector machines, to determine which algorithm is best suited for the task. We will also explore the use of ensemble methods, such as random forests and gradient boosting, to improve the accuracy of our model.\n",
    "\n",
    "#### Model Evaluation\n",
    "!!! TO BE UPDATED !!!\n",
    "We will evaluate the performance of our model using a variety of metrics, such as accuracy, precision, recall, and F1-score. We will also use techniques such as cross-validation to ensure that our model is not overfitting the training data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "We propose to use a Linear Mixed-Effects Model (LMM) to predict the political affiliation (Republican or Democrat) of tweet authors, incorporating both the textual content of the tweet and associated numerical features such as the number of retweets, likes, and replies. The LMM will allow us to model the fixed effects of the tweet's textual content and numerical features, while accounting for the random effects of individual tweet authors. By including interaction terms between the textual and numerical features, we can capture the non-linear relationships between them and improve the accuracy of our predictions.\n",
    "\n",
    "To determine the best set of parameters and hyperparameters for the LMM, we will use cross-validation and grid search techniques. We will divide our dataset into training, validation, and testing sets, and use the training set to fit different LMMs with varying combinations of parameters and hyperparameters. We will then use the validation set to evaluate the performance of each model and select the one with the highest accuracy. Finally, we will test the performance of the selected model on the testing set to ensure its generalizability.\n",
    "\n",
    "In addition, we will use the Bayesian Information Criterion (BIC) from statsmodels library to evaluate the fit of different models and select the one with the lowest BIC score. We can also examine the estimated coefficients and p-values for each term in the model to determine their significance and contribution to the prediction of political affiliation. By carefully selecting and tuning our parameters and hyperparameters, we aim to develop an accurate and robust LMM for predicting the political affiliation of tweet authors based on both textual and numerical features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "One evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model is the F1 score. In the context of predicting tweet engagement, precision represents the proportion of predicted viral tweets that are actually viral, while recall represents the proportion of actual viral tweets that are correctly predicted as viral. The F1 score balances these two metrics, giving equal weight to precision and recall, and provides a single score that summarizes the overall performance of the model.\n",
    "\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "where precision = true positives / (true positives + false positives) and recall = true positives / (true positives + false negatives)\n",
    "\n",
    "True positives are the number of tweets that are correctly predicted as viral, while false positives are the number of tweets that are predicted as viral but are actually non-viral, and false negatives are the number of tweets that are actually viral but are predicted as non-viral. For example, if the solution model correctly predicts 80% of viral tweets with a precision of 75%, and correctly predicts 70% of non-viral tweets with a precision of 90%, then the F1 score would be:\n",
    "\n",
    "precision = 0.75 recall = 0.8 F1 = 2 * (0.75 * 0.8) / (0.75 + 0.8) = 0.77\n",
    "\n",
    "This indicates that the model has a relatively good balance between precision and recall, with a reasonable overall performance. By comparing the F1 score of the benchmark model and the solution model, we can assess the effectiveness of the solution in improving predictive accuracy.\n",
    "\n",
    "After developing our LMM with the best set of parameters and hyperparameters, we will compare our proposed LMM model with the benchmark model, which is a multi-nomial model that uses both textual and numerical features to predict the political affiliation of tweet authors. We will evaluate the performance of both models using metrics such as accuracy, precision, recall, and F1-score, and use statistical tests such as t-tests or ANOVA to determine if there is a significant difference in their performance. By comparing our LMM model with the benchmark model, we aim to demonstrate the effectiveness of our proposed approach in capturing the non-linear relationships between textual and numerical features and improving the accuracy of political affiliation predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary results\n",
    "\n",
    "NEW SECTION!\n",
    "\n",
    "After reading about the best algorithms for natural language processing classification, we decided the baseline algorithm against which others should be compared will be multinomial naive Bayes. As such, we trained a baseline model using all 75% of our data and reserved the other 25% as a test set. The results are below\n",
    "\n",
    "\n",
    "Classification Report\n",
    "======================================================\n",
    "\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.81      0.87      0.84    113665\n",
    "           1       0.80      0.71      0.76     82902\n",
    "\n",
    "    accuracy                           0.81    196567\n",
    "   macro avg       0.81      0.79      0.80    196567\n",
    "weighted avg       0.81      0.81      0.80    196567\n",
    "\n",
    "\n",
    "\n",
    "As we can see, we achieved a promising level of overall accuracy (81%), but experienced very different precision and recall scores for our two categories. While this is a nice baseline, we would like to achieve both higher accuracy and similar precision and recall for our two labels. \n",
    "\n",
    "\n",
    "As our data collection and processing was quite time intensive, we have not yet begun to analyze the effect of other models, nor vary the number of features extracted in our text processing portion. In addition, we would like to compare different sizes of training sets and testing sets, as this model was only trained on around 15% of our data for the sake of time. However, with the coding intensive portions of this project behind us, we can turn our full focus towards finding the best combination of models and features to perform our task.\n",
    "\n",
    "Notebook running Naive Bayes Model on dataset: https://github.dev/COGS118A/Group040-Wi23/blob/457fde032807ede24fc8b8c71c0746237bc45347/algorithm_application.ipynb (includes the preprocessing of text data for best model results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If someone were to use such an algorithm to identify the partisan affiliation of someone they know based on their Twitter activity, it could lead to that individual being singled out and potentially discriminated against. For example, they might face social ostracism or even job discrimination based on their political views, which is a clear violation of their rights to free speech and privacy.\n",
    "\n",
    "This is precisely why we have secret ballots in the US. The secret ballot ensures that citizens can express their political views without fear of retaliation or discrimination. It allows individuals to vote for the candidate or party that they truly support without the risk of being singled out or punished for their political views.\n",
    "\n",
    "In the case of using Twitter data to predict political affiliations, it is important to consider the potential implications of such models on individuals' privacy and rights. While it can be tempting to use this kind of data to gain insights into people's political views, it is important to approach it with caution and respect for individuals' privacy.\n",
    "Data Collection and Usage: One important ethical consideration is how the data is collected and used. For example, is the data being collected with the user's consent? Is it being used for purposes other than what was initially intended? It's important to ensure that data is collected and used in an ethical and transparent manner. While these tweets are obtained legally from politicians who know what they're doing when they post online , there's an implication that the average joe could have their data being used against them. \n",
    "\n",
    "Bias in Predictive Models: Another ethical concern is the potential for bias in predictive models. These models may be trained on biased data, which could result in inaccurate predictions and reinforce existing biases. It's important to test and validate predictive models to ensure that they are fair and accurate.\n",
    "\n",
    "Role of Social Media Companies: Social media companies play a key role in collecting and sharing data. It's important for these companies to have policies and practices in place to protect users' data and privacy. This includes being transparent about how data is collected and used, and providing users with options to control their data.\n",
    "\n",
    "Our dataset is publicly avaiable. So are a huge chunk of tweets thanks to twitter's API. There's not much we can do to prevent the data from falling into the wrong hands, the bigger concern is the model being misuses. \n",
    "\n",
    "How are we going to handle ethics ? \n",
    "\n",
    "**Model training and evaluation**: We will use techniques such as cross-validation to evaluate the performance of the model and identify any biases that may exist. \n",
    "\n",
    "**Deployment and monitoring**: We will monitor the performance of the model in production and identify any unintended consequences or ethical issues that may arise. We will also implement mechanisms to prevent the misuse of the model, such as restricting access to the model or using it only for specific purposes.\n",
    "\n",
    "**Regular review**: We will regularly review the model's performance and re-evaluate its ethical implications, as well as update the model to address any new concerns that may arise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put things here that cement how you will interact/communicate as a team, how you will handle conflict and difficulty, how you will handle making decisions and setting goals/schedule, how much work you expect from each other, how you will handle deadlines, etc...\n",
    "\n",
    "- Cameron Faulkner: Twitter Data Collection\n",
    "- Nikhil Hegde: training model\n",
    "- Qianxi Gong: Implementation of sentiment analysis and feature parsing\n",
    "- Atul Nair: Model evaluation and selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2/28: finish data collection\n",
    "\n",
    "3/5/23: finish model construction\n",
    "\n",
    "3/8: complete checkpoint requirements\n",
    "\n",
    "3/12: conduct model selection and analysis\n",
    "\n",
    "3/15: begin final write up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^] Antypas D. , Preece A., Camacho-Collados J. “Politics, Sentiment and Virality: A Large-Scale Multilingual Twitter Analysis in Greece, Spain and United Kingdom”. Online Social Networks and Media. August 22, 2022. https://arxiv.org/pdf/2202.00396.pdf\n",
    "\n",
    "<a name=\"admonishnote\"></a>2.[^] Gangwar, A. and Mehta,T.. “Sentiment Analysis of Political Tweets for Israel using Machine Learning”. LearnByResearch. April, 2022. https://arxiv.org/pdf/2204.06515.pdf\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
