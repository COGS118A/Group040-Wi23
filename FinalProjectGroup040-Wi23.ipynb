{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Political Affiliation Classification "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group members\n",
    "\n",
    "- Cameron Faulkner\n",
    "- Nikhil Hegde\n",
    "- Qianxi Gong\n",
    "- Atul Nair"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents \n",
    "- the solution/what you did\n",
    "- major results you came up with (mention how results are measured) \n",
    "\n",
    "Twitter has become a popular platform for political discourse, where people express their views on a variety of issues. Given the increasing polarization in the political landscape, it is of great interest to understand how people's political views are reflected in their online behavior. Our goal is to develop a model that accurately predicts whether a tweet is posted by a Republican or Democrat, taking into account both the textual content of the tweet and associated numerical features such as the number of retweets, likes, and followers. We seek to identify the best set of features and models that can effectively capture the relationship between the text and numerical features and political affiliation of tweet authors.\n",
    "\n",
    "__NB:__ this final project form is much more report-like than the proposal and the checkpoint. Think in terms of writing a paper with bits of code in the middle to make the plots/tables"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Based on existing literature , Several studies have shown promising results using supervised learning models for political affiliation classification based on Twitter data. The results of these studies showed that some models can predict partisan affiliation, with accuracy rates ranging from around 60% to over 90% depending on the specific model and dataset used. For example, in a study by Conover et al. \n",
    "(2011), a logistic regression model achieved an accuracy of 86.4% in predicting political orientation (liberal vs. conservative) based on Twitter data [1]. In another study by Barberá et al. (2015), a Random Forest model achieved an accuracy of 87% in predicting US congressional districts' partisan affiliation [2]. They found that incorporating information on retweets and mentions, in addition to tweet text, significantly improved the accuracy of their models [2]. This suggests that the choice of features can play a critical role in the effectiveness of these models.However, an unanswered question in this field is the generalizability of these models to different political contexts or populations. For instance, a study by Mocanu et al. (2015) showed that supervised learning models trained on tweets from the UK Independence Party (UKIP) members may not generalize well to other political groups, such as the Labour Party [3]. Similarly, the effectiveness of these models may vary across different geographical regions, cultures, or languages.\n",
    "\n",
    "Therefore, while the literature shows promising results for partisan affiliation prediction using supervised machine learning on Twitter, it is important to take into consideration the specific political context and the selection of features to ensure the generalizability and robustness of these models.\n",
    "We looked at existing research for background information , but our ideas were mostly internally conceptualized. On obtaining the dataset , which had several excellent features , we managed to use our own intuition to build and train the models. The studies cited were mostly background information to give us an idea about the possible pitfalls of our own model\n",
    "\n",
    "\n",
    "[1] Conover, M. D., Ratkiewicz, J., Francisco, M., Gonçalves, B., Menczer, F., & Flammini, A. (2011). Political polarization on twitter. In Proceedings of the Fifth International AAAI Conference on Weblogs and Social Media (pp. 89-96).\n",
    "[2] Pak, A., & Paroubek, P. (2010). Twitter as a corpus for sentiment analysis and opinion mining. In LREc (pp. 1320-1326).\n",
    "[3] Mocanu, D., Baronchelli, A., Perra, N., Gonçalves, B., Zhang, Q., & Vespignani, A. (2015). The Twitter of Babel: Mapping world languages through microblogging platforms. PloS one, 10(11), e0142968. doi: 10.1371/journal.pone.0142968"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Our goal is to develop a Machine Learning model that predicts whether a tweet is posted by a Republican or Democrat, taking into account both the textual content of the tweet using Semantic Analysis and NLP motivated techniques and associated numerical features such as the number of retweets, likes, and followers. We seek to identify the best set of features and models that can effectively capture the relationship between the text and numerical features and political affiliation of tweet authors which would result in the most accurate predictions using out Evaluation Metrics described below. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "In order to train our predictive model, we scraped every Tweet made by a member of the American Congress with a Twitter account during the 117th session of Congress, which ran from January 3rd, 2021 to January 3rd, 2023. In total, 530 of 535 members of this session of Congress had Twitter accounts.\n",
    "\n",
    "We are aware this small collection of users could bias the data. If we were to work with a subset of random Twitter users instead, it would be difficult to manually identify hundreds of Twitter accounts whether they belong to an American political party, or show liberal or conservative viewpoints. Additionally, filtering through bot accounts pose another difficult challenge. \n",
    "\n",
    "Gathering data from verified Twitter accounts of members of congress allows us to easily identify political leanings as well and we can extrapolate our findings to the rest of the population as these members of Congress are the public figures of the party. \n",
    "\n",
    "Using the dataset of Congressional Twitter handles, we utilized the snscrape Python package to scrape all of the Tweets in our specified time frame from each of the members. This amounted to 786268 Tweets (clearly, members of Congress are prolific Tweeters). [Link to the unprocessed scraped Tweets](https://drive.google.com/file/d/1IUHw2ktOTZc41vi_7Ijrw20-maLJfjTU/view?usp=share_link)\n",
    "\n",
    "The median amount of Tweets sent by a member of Congress over this span was 1336, corresponding to approximately 1.83 Tweets per day. \n",
    "\n",
    "\n",
    "### Removal of Irrelevant Features\n",
    "For each Tweet in our dataset, we kept the following variables and eliminated some we found irrelevant:\n",
    "\n",
    "**KEPT VARIABLES**\n",
    "- `rawContent`: the content of the tweet itself generated by the twitter user \n",
    "- `replyCount`: number of replies\n",
    "- `retweetCount`: number of retweets \n",
    "- `likeCount`: number of likes \n",
    "- `quoteCount`: number of quotes \n",
    "- `links`: any files or resources hyperlinked into the tweet\n",
    "- `Media`: any videos embedded into the tweet\n",
    "- `mentionedUsers`: the users mentioned in the tweet \n",
    "- `Hashtags`: hashtags appended to the end of the tweet\n",
    "- `Party`: automated party identification of each member of congress during the data collection process\n",
    "\n",
    "We elected to drop the following variables as we found them to be redundant or of little utility to our project:\n",
    "\n",
    "**DROPPED VARIABLES**\n",
    "- `Url`\n",
    "- `Date`\n",
    "- `Id`\n",
    "- `User`\n",
    "- `renderedContent`\n",
    "- `Coordinates`\n",
    "- `place`\n",
    "- `Card`\n",
    "- `viewCount`\n",
    "- `Vibe`\n",
    "\n",
    "\n",
    "However, we only used these variables as a starting point for our analysis and needed to treat some of them so that they were usable in our dataset. \n",
    "\n",
    "### Preprocessing and Addition of Features\n",
    "\n",
    "#### TREATMENT OF VARIABLES TAKEN FROM TWITTER SCRAPING\n",
    "\n",
    "While all of our variables with the word `Count` in them were already in numerical format, we needed to transform the others into numerical data.\n",
    "\n",
    "For the variables `links`, `Media`, `mentionedUsers`, and `Hashtags`, we weren’t interested in their actual content, but rather how many of them were present in a given tweet (e.g. the number of links placed in a single Tweet). As such, we transformed each variable for every observation into the length of the list of items it pertained to yielding values from 0 and up. \n",
    "\n",
    "For `Party` we changed our initial classification of `R` or `D` into 1 and 0, respectively, as we knew that we cannot feed a model categorical data in the form of text. We will be using the party label as the basis of our classifier.\n",
    "\n",
    "As a fortunate result of using this particular scraper, all of our data was well formatted (no strings were in fields reserved for floats, for example) and, after doing our best practice checks on our data, we found no observations needed to be thrown out.\n",
    "\n",
    "[link to notebook with implementation of cleaning process described before](https://github.dev/COGS118A/Group040-Wi23/blob/7ee24d6fe6ac05b2b2f63b75e9e21c35270c9291/tweet_cleaning.ipynb)\n",
    "\n",
    "\n",
    "#### ADDITION OF DERIVED VARIABLES\n",
    "\n",
    "**Sentiment Analysis**\n",
    "\n",
    "In addition to the variables provided by our scraping, we conducted sentiment analysis on the text content of each of our Tweets with the theory that Republicans and Democrats may differ in the tone of their communication with the public. \n",
    "\n",
    "To apply our sentiment analysis, we took the rawContent variable for each of our Tweets (literally the text of the Tweet) and applied the Vader Sentiment analysis algorithm to them. This algorithm is specifically for the sentiment classification of social media posts, an important factor to consider given how different a Tweet is from a novel. We used this model to classify the sentiment of each Tweet as “positive”, “negative”, or “neutral” based on the scores provided by the algorithm and the thresholds recommended by the developers of the sentiment analysis package. \n",
    "\t\n",
    "Again, we ran into the problem of using categorical data in an ML algorithm and so we used one-hot encoding to create three separate binary variables corresponding to each of the three sentiments. \n",
    "\n",
    "[link to notebook with sentiment analysis addition](https://github.dev/COGS118A/Group040-Wi23/blob/7ee24d6fe6ac05b2b2f63b75e9e21c35270c9291/sentiment%20analysis%20addition.ipynb)\n",
    "\n",
    "\n",
    "**Word Frequency and Importance**\n",
    "\n",
    "We also decided to take further advantage of the rich text content of our dataset and include the frequency and uniqueness of the words that comprised the Tweets in our dataset under the hypothesis that Republicans and Democrats used certain words more frequently than the other party. \n",
    "\n",
    "To do so, we used the standard Term Frequency-Inverse Document Frequency measure to quantify the significance of the words used in our Tweets. Understanding that using the algorithm on unprocessed data can yield falsely significant results, we knew we must preprocess the data before applying TF-IDF. We removed stop words, words that commonly occur and offer no value to our classification, using the Natural Language Tool Kit’s (nltk) premade stopwords corpus, and also applied nltk’s Lemmatizer which removes the conjugation of words to avoid duplicates and diluting the significance of them. \n",
    "\n",
    "We then took our processed Tweets and vectorized them with sklearn’s TfidfVectorizer and extracted 4500 features from our dataset. The resulting matrix was in the form of columns that contained the TF_IDF scores for each of the 4500 features, for every one of the Tweets. This matrix, added to our 11 other variables, formed our completed dataset.\n",
    "\n",
    "[link to notebook with text processing and vectorization, and model application](https://github.dev/COGS118A/Group040-Wi23/blob/2a99aec348b73c5eda78ccdd1974db821d0badca/algorithm_application.ipynb)\n",
    "\t\n",
    "    \n",
    "**COMPLETED DATASET**\n",
    "\n",
    "In sum, the dataset we are using for our model has the dimensions of (786268, 4513). This includes the following features: sentiment, engagement metrics from Twitter users, word similarity, and party labels from the Member of Congress who made a given Tweet that will serve as our classification labels.\n",
    "\n",
    "[Link to completed dataset on Google Drive](https://drive.google.com/file/d/1Z5hXQE7qe--6Cpk_8biCfP_sHRZy7XXb/view?usp=sharing)\n",
    "\n",
    "Sources/Packages Used:\n",
    "\n",
    "- [Congressional Twitter Handles](https://frac.org/wp-content/uploads/MOC_Twitter-Handles_117th.pdf)\n",
    "\n",
    "- [VADER Sentiment Analysis Documentation](https://github.com/cjhutto/vaderSentiment)\n",
    "\n",
    "- [NLTK Stopwords Documentation](https://www.nltk.org/search.html?q=stopwords&check_keywords=yes&area=default)\n",
    "\n",
    "- [NLTK Lemmatizer Documentation](https://www.nltk.org/_modules/nltk/stem/wordnet.html)\n",
    "\n",
    "- [SKLearn TFidfVectorizer Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "<!-- In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Make sure to describe how the solution will be tested.  \n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared.  -->\n",
    "\n",
    "\n",
    "We propose to use a Naive Bayes model to predict the political affiliation (Republican or Democrat) of tweet authors, incorporating both the textual content of the tweet and associated numerical features such as the number of retweets, likes, and replies. The Naive Bayes will allow us to model the fixed effects of the tweet's textual content and numerical features, while accounting for the random effects of individual tweet authors. By including interaction terms between the textual and numerical features, we can capture the non-linear relationships between them and improve the accuracy of our predictions.\n",
    "\n",
    "To determine the best set of parameters and hyperparameters for the Naive Bayes, we will use k fold cross-validation and grid search techniques. We will divide our dataset into training, validation, and testing sets, and use the training set to fit different Naive Bayes with varying combinations of parameters and hyperparameters. We will then use the validation set to evaluate the performance of each model and select the one with the highest accuracy. Finally, we will test the performance of the selected model on the testing set to ensure its generalizability.\n",
    "\n",
    "By carefully selecting and tuning our parameters and hyperparameters, we aim to develop an accurate and robust Naive Bayes for predicting the political affiliation of tweet authors based on both textual and numerical features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "<!-- !! UPDATE WITH EXAMPLE !!\n",
    "Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms). -->\n",
    "\n",
    "\n",
    "To evaluate the models, we use the Receiver Operating Characteristic (ROC) curve to evaluate the model's performance. \n",
    "<!-- \n",
    "The Precision-Recall curve is a graphical representation of the precision and recall values of the classifier. Precision is the ratio of correctly predicted positive instances to the total number of predicted positive instances.\n",
    "\n",
    "$$ Precision = \\frac{TP}{TP+FP}  $$ \n",
    "\n",
    "Recall is the ratio of correctly predicted positive instances to the total number of actual positive instances. \n",
    "\n",
    "$$ Recall = \\frac{TP}{TP+FN} $$\n",
    "\n",
    "The Precision-Recall curve would be useful for evaluating the model's performance when dealing with imbalanced datasets, as it focuses on the precision and recall of the positive class. -->\n",
    "\n",
    "\n",
    "The ROC curve is a graphical representation of the true positive rate (TPR) against the false positive rate (FPR) for varying classification thresholds. The TPR is the ratio of correctly predicted positive instances to the total number of actual positive instances.\n",
    "\n",
    "$$ Sensitivity = TPR = \\frac{TP}{TP+FN} $$\n",
    "\n",
    "FPR is the ratio of incorrectly predicted positive instances to the total number of actual negative instances. \n",
    "\n",
    "$$ Specificity = 1 - FPR = 1 - \\frac{FP}{TN+FP} $$\n",
    "\n",
    "The area under the ROC curve (ROC AUC) provides an indication of the model's overall performance. A higher ROC AUC score indicates better performance, and a lower score indicates poorer performance.The ROC curve would be useful for evaluating the model's performance when the cost of false positives and false negatives are roughly equal, as it focuses on the TPR and FPR of both classes.\n",
    "\n",
    "For example, here is the ROC Curve for our benchmark Naive Bayes Algorithm \n",
    "\n",
    "![ROC](./images/ROC.png)\n",
    "\n",
    "we were able to get an ROC score of 0.75. The closer we get this metric to 1, the better our model will perform. \n",
    "\n",
    "<!-- ![cm](./images/cm_example.png)\n",
    "\n",
    "We can calculate the FPR and TPR as\n",
    "\n",
    "$$ FPR_{example} = \\frac{TP}{TP+FP} = \\frac{5100}{5100+1500} = 0.77  $$\n",
    "\n",
    "$$ TPR_{example} = \\frac{TP}{TP+FN} = \\frac{5100}{5100+3300} = 0.60  $$ -->\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Maybe you include a learning curve to show whether you have enough data to do train/validate/test split or have to go to k-folds or LOOCV or ???\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "If someone were to use such an algorithm to identify the partisan affiliation of someone they know based on their Twitter activity, it could lead to that individual being singled out and potentially discriminated against. For example, they might face social ostracism or even job discrimination based on their political views, which is a clear violation of their rights to free speech and privacy. This is precisely why we have secret ballots in the US. The secret ballot ensures that citizens can express their political views without fear of retaliation or discrimination. It allows individuals to vote for the candidate or party that they truly support without the risk of being singled out or punished for their political views. In the case of using Twitter data to predict political affiliations, it is important to consider the potential implications of such models on individuals' privacy and rights. While it can be tempting to use this kind of data to gain insights into people's political views, it is important to approach it with caution and respect for individuals' privacy.\n",
    "\n",
    "**Data Collection and Usage:** One important ethical consideration is how the data is collected and used. For example, is the data being collected with the user's consent? Is it being used for purposes other than what was initially intended? It's important to ensure that data is collected and used in an ethical and transparent manner. While these tweets are obtained legally from politicians who know what they're doing when they post online , there's an implication that the average joe could have their data being used against them. \n",
    "\n",
    "**Bias in Predictive Models:** Another ethical concern is the potential for bias in predictive models. These models may be trained on biased data, which could result in inaccurate predictions and reinforce existing biases. It's important to test and validate predictive models to ensure that they are fair and accurate.\n",
    "\n",
    "**Role of Social Media Companies:** Social media companies play a key role in collecting and sharing data. It's important for these companies to have policies and practices in place to protect users' data and privacy. This includes being transparent about how data is collected and used, and providing users with options to control their data.\n",
    "\n",
    "**Our dataset is publicly avaiable.** So are a huge chunk of tweets thanks to twitter's API. There's not much we can do to prevent the data from falling into the wrong hands, the bigger concern is the model being misuses. \n",
    "\n",
    "#### How are we going to handle ethics ? \n",
    "\n",
    "**Model training and evaluation**: We will use techniques such as cross-validation to evaluate the performance of the model and identify any biases that may exist. \n",
    "\n",
    "**Deployment and monitoring**: We will monitor the performance of the model in production and identify any unintended consequences or ethical issues that may arise. We will also implement mechanisms to prevent the misuse of the model, such as restricting access to the model or using it only for specific purposes.\n",
    "\n",
    "**Regular review**: We will regularly review the model's performance and re-evaluate its ethical implications, as well as update the model to address any new concerns that may arise.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren’t Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
